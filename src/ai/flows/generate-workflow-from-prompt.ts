
// The directive tells the Next.js runtime to execute this code on the server.
'use server';

/**
 * @fileOverview AI-powered workflow generator from a plain-text prompt.
 *
 * - generateWorkflowFromPrompt - A function that takes a plain-text workflow description and returns a JSON representation of the workflow.
 * - GenerateWorkflowFromPromptInput - The input type for the generateWorkflowFromPrompt function.
 * - GenerateWorkflowFromPromptOutput - The return type for the generateWorkflowFromPrompt function, which represents the workflow definition.
 */

import {ai} from '@/ai/genkit';
import {z} from 'zod';
import type { RetryConfig, BranchConfig, OnErrorWebhookConfig } from '@/types/workflow'; // Import RetryConfig type

// Define the schema for a node in the workflow.
const WorkflowNodeSchema = z.object({
  id: z.string().describe('Unique identifier for the node (e.g., "node_1", "fetch_data_api"). Use snake_case.'),
  type: z.string().describe('Type of the node (e.g., httpRequest, parseJson, aiTask, sendEmail, databaseQuery, conditionalLogic, executeFlowGroup, forEach, whileLoop, parallel, manualInput, callExternalWorkflow, fileSystemTrigger, googleCalendarListEvents, delay, webhookTrigger, getEnvironmentVariable, googleSheetsAppendRow, slackPostMessage, openAiChatCompletion, stripeCreatePaymentLink, hubspotCreateContact, twilioSendSms, githubCreateIssue, dropboxUploadFile, toUpperCase, toLowerCase, concatenateStrings, stringSplit, formatDate).Choose the most appropriate type. Refer to available node types and their specific config requirements.'),
  name: z.string().optional().describe('A descriptive name for the node instance (e.g., "Fetch User Profile", "Summarize Article"). Keep it concise.'),
  description: z.string().optional().describe('A brief description of what this specific node instance does or its purpose in the workflow.'),
  position: z.object({
    x: z.number(),
    y: z.number(),
  }).describe('The X, Y coordinates of the node in the visual editor. Nodes should be laid out logically. General flow: left-to-right or top-to-bottom. For sequences, stagger nodes slightly (e.g., y = previous.y + NODE_HEIGHT + 40). For conditional branches: place the conditional node, then position the "true" path downwards and to one side (e.g., x = conditional.x - NODE_WIDTH - 60, y = conditional.y + NODE_HEIGHT + 40) and the "false" path downwards and to the other side (e.g., x = conditional.x + NODE_WIDTH + 60, y = conditional.y + NODE_HEIGHT + 40) to create a clear tree-like structure. For parallel nodes, lay out their internal branches distinctly. Avoid overlaps. Start triggers near (50,50). A typical node is 200px wide (NODE_WIDTH) and 100px high (NODE_HEIGHT).'),
  config: z.any().describe("Configuration parameters for the node. This should be a valid JSON object. Populate relevant fields based on the node's 'type' and its corresponding 'configSchema'. For nodes that interact with external systems (like 'httpRequest', 'sendEmail', 'databaseQuery', 'aiTask', 'callExternalWorkflow', 'fileSystemTrigger', 'googleCalendarListEvents', 'webhookTrigger'), include corresponding simulation fields (e.g., 'simulatedResponse', 'simulatedOutput', 'simulatedFileEvent', 'simulatedEvents', 'simulatedRequestBody', 'simulatedRequestHeaders', 'simulatedRequestQuery', 'simulated_config' for new integrations) to enable testing in a simulation mode without making actual external calls. These fields provide mock data that the node will output during simulation. If crucial operational parameters (e.g., specific IDs, filename patterns, channel names, API keys) are needed but not explicit in the user prompt, PREFER '{{credential.USER_FRIENDLY_NAME_HERE}}' for managed credentials (e.g., '{{credential.MyOpenAIKey}}'), or use descriptive placeholders like '{{env.SERVICE_API_KEY}}' for environment variables (e.g., 'apiKey: \"{{credential.OpenAI_API_Key}}\"', 'targetFolder: \"{{env.TARGET_UPLOAD_FOLDER}}\"' or 'channelId: \"NEEDS_CONFIGURATION_CHANNEL_ID\"'). Explicitly state the requirement for user setup (including how to get the value, or that it needs to be set in the Credential Manager) in the 'aiExplanation'. Examples:\n- 'httpRequest': include 'url', 'method'. Optionally 'simulatedResponse' (JSON string or object for the response body) and 'simulatedStatusCode' (number, e.g., 200, 404, 500 for definitive status simulation) for simulation mode. Node output includes 'status_code', 'response', and on error: 'status: \"error\"', 'error_message'.\n- 'aiTask': include 'prompt', 'model'. Optionally 'simulatedOutput' (string) for simulation mode. Node output includes 'output', and on error: 'status: \"error\"', 'error_message'.\n- 'sendEmail': include 'to', 'subject', 'body'. Optionally 'simulatedMessageId' (string) for simulation mode. Expect EMAIL_* env vars for server settings. Node output includes 'messageId', and on error: 'status: \"error\"', 'error_message'.\n- 'databaseQuery': include 'queryText' (SQL with $1, $2 placeholders) and 'queryParams' (JSON array of values). Optionally 'simulatedResults' (JSON array) and 'simulatedRowCount' (number) for simulation mode. Expect DB_CONNECTION_STRING env var or a '{{credential.MyDatabaseConnection}}'. Node output includes 'results', 'rowCount', and on error: 'status: \"error\"', 'error_message'.\n- 'googleCalendarListEvents': include 'maxResults' (optional number). CRITICALLY include 'simulatedResponse' (JSON array, e.g., '[{\"summary\":\"My Event\"}]') for simulation mode, as real OAuth is not yet fully implemented. Node output includes 'events', 'status', 'error_message'.\n- 'conditionalLogic': 'condition' string (e.g., '{{data.value}} == \"success\"', '{{data.count}} > 10', '{{prev_node.status}} == \"error\"'). Outputs a boolean 'result'. This 'result' is typically used in a subsequent node's '_flow_run_condition' field (e.g., '_flow_run_condition: \"{{conditional_node_id.result}}\"') to control its execution.\n- 'executeFlowGroup': includes 'flowGroupNodes' (JSON array of valid WorkflowNode objects defining the sub-flow; IDs within this group must be unique to the group), 'flowGroupConnections' (JSON array of valid WorkflowConnection objects for the sub-flow nodes), 'inputMapping' (JSON object to map parent scope data to group scope, e.g., '{\"internalUserId\": \"{{trigger_node.userId}}\", \"productInfo\": \"{{product_fetch_node.response}}\"}'), and 'outputMapping' (JSON object to map group scope data to parent output, e.g., '{\"processedOrder\": \"{{group_final_process_node.final_order_data}}\", \"summaryText\": \"{{group_summary_node.text_output}}\"}'). Outputs include mapped values, 'status', 'error_message'.\n- 'forEach': includes 'inputArrayPath' (placeholder for the array, e.g., '{{api_node.users}}'), 'iterationNodes' (array of node definitions for the sub-flow), 'iterationConnections' (array of connections for the sub-flow), optionally 'iterationResultSource' (placeholder like '{{last_sub_node.output}}' to specify what to collect from each iteration), and optionally 'continueOnError' (boolean, default false; if true, loop continues if an iteration errors, results will show individual statuses: 'status: \"fulfilled\", value: ...' or 'status: \"rejected\", reason: ..., item: ...'. The node's overall status can be 'success', 'partial_success', or 'error'.). Inside 'iterationNodes' config, use '{{item.property}}' to access properties of the current item being looped over. Output: 'results' (array of iteration results/statuses), 'status', 'error_message'.\n- 'whileLoop': includes 'condition' (string placeholder, e.g., '{{api_node.response.hasNextPage}} === true', '{{counter.value}} < 5'), 'loopNodes' (array of node definitions for the sub-flow), 'loopConnections' (array of connections for the sub-flow), and 'maxIterations' (optional number, default 100). The condition is evaluated *before* each iteration. Nodes within 'loopNodes' should eventually modify data that the 'condition' depends on. Output: 'iterations_completed', 'status', 'error_message'.\n- 'parallel': includes 'branches' (JSON array of branch definitions) and an optional 'concurrencyLimit' (number, 0 or less means no limit). Each branch definition is an object: { 'id': 'branch_unique_id', 'name': 'Optional Name', 'nodes': [...], 'connections': [...], 'inputMapping': {optional JSON object to map parent scope data to this specific branch's scope, e.g., '{\"branchSpecificData\": \"{{parent_node.output}}\"}'}, 'outputSource': 'optional_placeholder_from_branch_scope indicating what data from this branch is its primary result, e.g., '\"{{last_node_in_branch.processed_data}}\"' }. Output: 'results' (object keyed by branch IDs, values are {status, value/reason}), 'status', 'error_message'.\n- 'manualInput': For representing human interaction. Config must include: 'instructions' (text for user), 'inputFieldsSchema' (JSON array describing form fields: {id, label, type, options?, defaultValue?, placeholder?, required?}), and CRITICALLY 'simulatedResponse' (JSON object for the data this node will output during simulation, which becomes available on its 'output' handle - **you MUST generate a plausible JSON for this field**). In a production system capable of pause/resume, this node would typically halt workflow execution and await actual user input. The 'simulatedResponse' is primarily for testing and for environments where a full pause/resume mechanism is not available or not desired for a particular step. Output: 'output' (the parsedSimulatedResponse), 'status', 'error_message'.\n- 'callExternalWorkflow': Has an 'input' handle for visual connection and 'output', 'status', 'error_message' output handles. Config must include: 'calledWorkflowId' (string), 'inputMapping' (JSON object to map current workflow data to called workflow inputs), 'outputMapping' (JSON object where keys are names for outputs this node will produce, and values are placeholders like '{{calledWorkflow.property}}' that resolve *directly against 'simulatedOutput'* to populate this node's 'output' handle), and CRITICALLY 'simulatedOutput' (JSON object representing the data this node will output to simulate the called workflow's execution; this is the direct data source that 'outputMapping' placeholders will reference - **you MUST generate a plausible JSON for this field**). \n- 'fileSystemTrigger': Config includes 'directoryPath', 'eventTypes' (JSON array), 'fileNamePattern' (optional), 'pollingIntervalSeconds' (optional), and CRITICALLY 'simulatedFileEvent' (JSON object or JSON string parsable to an object representing the mock file event data this trigger would conceptually produce - **you MUST generate a plausible JSON for this field**). In the current simulated environment, this node outputs the parsed 'simulatedFileEvent' data on its 'fileEvent' handle. This node has 'fileEvent', 'status', and 'error_message' output handles.\n- 'webhookTrigger': Config includes 'pathSuffix' (string, e.g., 'my-data-hook' which becomes part of the URL '/api/workflow-webhooks/my-data-hook'), optional 'securityToken' (string placeholder, e.g., '{{credential.MyWebhookSecret}}' or '{{env.MY_WEBHOOK_TOKEN}}'). CRITICALLY include 'simulatedRequestBody' (JSON object, e.g., {\"message\": \"Hello from webhook!\", \"userId\": \"user-sim-123\"}), 'simulatedRequestHeaders' (JSON object, e.g., {\"Content-Type\": \"application/json\", \"X-Test-Header\": \"SimulatedValue\"}), and 'simulatedRequestQuery' (JSON object, e.g., {\"source\": \"simulation\", \"version\": \"1.2\"}) for simulation mode. **You MUST generate plausible JSON examples for these fields.** These provide mock data for 'requestBody', 'requestHeaders', 'requestQuery' output handles during simulation. (Live execution will use actual incoming HTTP request data.) Output handles: 'requestBody', 'requestHeaders', 'requestQuery', 'status', 'error_message'.\n- 'getEnvironmentVariable': Config includes 'variableName' (string, e.g., 'MY_API_KEY') and 'failIfNotSet' (boolean, default false). Outputs 'value' (the env var value or null), 'status', 'error_message'.\n- Use placeholders like '{{previous_node_id.output_handle_name.property}}' to reference outputs. For credentials, PREFER '{{credential.USER_DEFINED_NAME}}' (e.g., '{{credential.MyOpenAIKey}}') for centrally managed credentials (mentioning the env var fallback for dev/testing, e.g. '{{env.OPENAI_API_KEY}}') or '{{env.YOUR_VARIABLE_NAME}}' for simple environment variables. The system may attempt to resolve '{{credential.A_PLACEHOLDER_CREDENTIAL_NAME}}' using a corresponding environment variable '{{env.A_PLACEHOLDER_CREDENTIAL_NAME}}' as a fallback for local development/testing if the Credential Manager isn't fully implemented or a specific credential isn't found, but in production, the Credential Manager is assumed for '{{credential...}}' placeholders. For conditional execution, if a node should only run if a certain condition (typically from a 'conditionalLogic' node's boolean 'result' output) is met, add a '_flow_run_condition' field to its 'config', like '_flow_run_condition: \"{{id_of_conditional_node.result}}\"'. The 'result' from 'conditionalLogic' MUST be a boolean.\n- Retry Configuration: For nodes like 'httpRequest', 'aiTask', 'sendEmail', 'databaseQuery', 'googleCalendarListEvents', 'getEnvironmentVariable', 'executeFlowGroup', 'forEach', 'whileLoop', 'parallel', 'manualInput', 'callExternalWorkflow', 'fileSystemTrigger', and 'webhookTrigger', you can add an optional 'retry' object to their 'config'. It has these fields (all optional): 'attempts' (number, total tries, e.g., 3), 'delayMs' (number, initial delay in milliseconds, e.g., 1000), 'backoffFactor' (number, for exponential delay, e.g., 2), 'retryOnStatusCodes' (array of numbers, for HTTP nodes, e.g., [500, 503, 429]), 'retryOnErrorKeywords' (array of strings, case-insensitive keywords to match in error messages, e.g., for an 'aiTask': ['timeout', 'quota exceeded', 'service unavailable']; for 'databaseQuery': ['connection refused', 'deadlock']). If specific conditions (status code or keywords) are set and *not* met by an error, no retry occurs even if attempts remain. Example: 'config': { ..., \"retry\": { \"attempts\": 3, \"delayMs\": 1000, \"backoffFactor\": 2, \"retryOnStatusCodes\": [500, 503, 429], \"retryOnErrorKeywords\": [\"timeout\", \"service unavailable\"] } }.\n- On-Error Webhook: For the same nodes that support 'retry', you can add an optional 'onErrorWebhook' object to their 'config'. Example: config: { ..., \"onErrorWebhook\": { \"url\": \"https://my-error-handler.com/notify\", \"method\": \"POST\", \"headers\": {\"X-API-Key\": \"{{env.MY_ERROR_KEY}}\"}, \"bodyTemplate\": { \"nodeId\": \"{{failed_node_id}}\", \"nodeName\": \"{{failed_node_name}}\", \"errorMessage\": \"{{error_message}}\", \"timestamp\": \"{{timestamp}}\", \"workflowSnapshot\": \"{{workflow_data_snapshot_json}}\" } } }. If the node fails after all retries, a request is sent to this webhook. Placeholders for 'bodyTemplate' and 'headers': '{{failed_node_id}}', '{{failed_node_name}}', '{{error_message}}', '{{timestamp}}', '{{workflow_data_snapshot_json}}' (full workflow data as JSON string). Env vars like '{{env.VAR_NAME_HERE}}' can be used in headers/bodyTemplate. This is a fire-and-forget notification. It can be used for simple alerts or to send error details to an endpoint that acts as a Dead-Letter Queue (DLQ) processor or triggers a dedicated error-handling workflow. The 'workflow_data_snapshot_json' payload is crucial for DLQ scenarios as it provides the complete context for later analysis or reprocessing."),
  aiExplanation: z.string().optional().describe("CRITICAL FOR USER GUIDANCE - BE EXTREMELY CLEAR, FRIENDLY, AND HIGHLY ACTIONABLE: Your detailed explanation for this node *must be friendly, clear, and highly actionable*. If this node requires any external configuration (API keys, specific IDs, server addresses, etc.) that are not part of the user's prompt: 1. Ensure a clear placeholder for this configuration exists in the node's 'config' (e.g., 'apiKey: \"{{credential.MyOpenAIKey}}\"' or 'apiKey: \"{{env.SERVICE_API_KEY}}\"'). 2. In this 'aiExplanation', EXPLICITLY state what information the user needs to provide, why it's needed, the placeholder used in the config, AND PROVIDE *simple, step-by-step instructions* OR VERY SPECIFIC HINTS on where/how to find or generate this value (e.g., 'This node requires an OpenAI API Key, configured as `{{credential.MyOpenAIKey}}`. Go to platform.openai.com, log in, navigate to API Keys, and generate a new secret key. Then, add this key in this application's Credential Manager under the name `MyOpenAIKey`. For development, you can temporarily set an OPENAI_API_KEY environment variable, which will be used if `MyOpenAIKey` is not found in the Credential Manager.' OR 'For the `{{env.DATABASE_PASSWORD}}`, consult your database administrator or your hosting provider's documentation for retrieval instructions. For the `{{env.SLACK_CHANNEL_ID}}`, open Slack, right-click on the desired channel name, select 'Copy link'. The last part of the URL (starting with C or G) is the Channel ID.'). Be specific if the service (e.g., OpenAI, Stripe, AWS S3) is commonly known. Also explain: why the node was chosen, its overall configuration (data flow placeholders, conditional execution using `_flow_run_condition: \"{{id_of_conditional_node.result}}\"` which expects a boolean and its source, specific config for complex nodes like 'sendEmail'/'databaseQuery'/'googleCalendarListEvents' [CRITICALLY, for 'googleCalendarListEvents', explain it needs 'simulatedResponse' (for which you MUST generate a plausible JSON array of event objects in its 'config', e.g., '[{\"summary\": \"Team Meeting\", \"start\": {\"dateTime\": \"YYYY-MM-DDTHH:mm:ssZ\"}}]') for current simulation and that real execution would require Google OAuth setup via the Credential Manager, or GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET env vars as fallback; also list its 'events', 'status', 'error_message' output handles]/executeFlowGroup [explaining node ID scoping within the group to be unique to that group, how 'inputMapping' brings parent data into the group scope, and how 'outputMapping' exposes group data to the parent]/forEach [explaining 'inputArrayPath', 'iterationNodes' with '{{item.property}}' usage for current item access and group-scoped unique IDs for nodes within 'iterationNodes', 'iterationConnections', optional 'iterationResultSource', and 'continueOnError' behavior including the structure of the 'results' array (individual statuses) and overall node status (success/partial_success/error) if 'continueOnError' is true]/whileLoop [explaining 'condition', 'loopNodes' with group-scoped unique IDs, 'loopConnections', 'maxIterations', and how loop state (data affecting the condition) is managed within the loop nodes and resolved against the parent workflow data scope]/parallel [explaining 'branch' definitions including each branch's 'id', 'nodes' with group-scoped unique IDs, 'connections', branch-specific 'inputMapping', 'outputSource', and optional 'concurrencyLimit' for the parent parallel node. The 'results' output is an object keyed by branch IDs]/manualInput [explaining 'instructions', 'inputFieldsSchema', and CRITICALLY 'simulatedResponse' (whose parsed content becomes the node's 'output' data - **ensure you generate a plausible JSON example for this in the config**) emphasizing its simulation nature in the current system vs. production pause/resume capabilities not covered here]/callExternalWorkflow [explaining 'calledWorkflowId', 'inputMapping' (how data flows from current workflow to called one), 'outputMapping' (how '{{calledWorkflow.property}}' placeholders reference keys *directly within the CRITICAL 'simulatedOutput' JSON* to populate this node's 'output' handle), and CRITICALLY 'simulatedOutput' (**ensure you generate a plausible JSON example for this in the config** and explain that this JSON structure is what 'outputMapping' references), plus its 'output', 'status', 'error_message' handles]/fileSystemTrigger [explaining 'directoryPath', 'eventTypes', 'fileNamePattern', 'pollingIntervalSeconds', CRITICALLY 'simulatedFileEvent' (**ensure you generate a plausible JSON example for this in the config**) and explain that its parsed content becomes data on the 'fileEvent' handle), and its 'fileEvent', 'status', 'error_message' handles]/delay [explaining 'delayMs' and that it passes through its input to output after delay]/'webhookTrigger' [CRITICALLY, explain its 'pathSuffix' (forms part of the URL `/api/workflow-webhooks/[pathSuffix]`), optional 'securityToken' (e.g., '{{credential.WebhookToken}}' or '{{env.WEBHOOK_TOKEN}}'), and that it needs 'simulatedRequestBody' (e.g., {\"message\": \"Hello from webhook!\", \"userId\": \"user-sim-123\"}), 'simulatedRequestHeaders' (e.g., {\"Content-Type\": \"application/json\"}), and 'simulatedRequestQuery' (e.g., {\"source\": \"simulation\", \"version\": \"1.2\"}) in its 'config' for simulation mode - **ensure you generate plausible JSON examples for these**. Explain that in live execution, its 'requestBody', 'requestHeaders', 'requestQuery' output handles are populated by the *actual incoming HTTP request data* from the API route. List all its output handles: 'requestBody', 'requestHeaders', 'requestQuery', 'status', 'error_message'.]/'getEnvironmentVariable' [explaining `variableName` and `failIfNotSet`, and its output handles `value`, `status`, `error_message`]), any 'retry' configuration (explaining fields: 'attempts', 'delayMs', 'backoffFactor', 'retryOnStatusCodes' for HTTP nodes, and versatile 'retryOnErrorKeywords' for case-insensitive matching of error messages like ['quota', 'timeout'] for an AI task), any 'onErrorWebhook' configuration (explaining potential uses like DLQ/error workflow integration and the importance of '{{workflow_data_snapshot_json}}'), and simulation-specific config like 'simulatedResponse'/'simulatedStatusCode' for 'httpRequest' or 'simulated_config' for new integration nodes. For nodes with simulation configs, explain their purpose for testing. Explain how workflow addresses error handling (using 'conditionalLogic' checks on 'status'/'error_message' or 'onErrorWebhook')."),
});

// Define the schema for a workflow connection (an edge in the graph). Describes how nodes are connected.
const WorkflowConnectionSchema = z.object({
  id: z.string().optional().describe('Unique identifier for the connection (e.g., "conn_1"). If not provided, one will be generated.'),
  sourceNodeId: z.string().describe('ID of the source node.'),
  sourcePort: z.string().optional().describe("The specific output port/handle on the source node to connect from (e.g., 'response', 'videos', 'result', 'output_data', 'status', 'error_message', 'results', 'iterations_completed', 'output' for manualInput/callExternalWorkflow/delay, 'fileEvent' for fileSystemTrigger, 'events' for googleCalendarListEvents, 'requestBody' for webhookTrigger, 'value' for getEnvironmentVariable). Refer to the source node type's 'outputHandles' definition. Nodes that can fail (httpRequest, aiTask, databaseQuery, sendEmail, googleCalendarListEvents, getEnvironmentVariable, executeFlowGroup, forEach, whileLoop, parallel, manualInput, callExternalWorkflow, fileSystemTrigger, webhookTrigger, delay) will output a 'status' (e.g., 'success', 'error', 'partial_success') and 'error_message' field if an error occurs."),
  targetNodeId: z.string().describe('ID of the target node.'),
  targetPort: z.string().optional().describe("The specific input port/handle on the target node to connect to (e.g., 'input', 'jsonData', 'input_data', 'input_array_data'). Refer to the target node type's 'inputHandles' definition."),
});

// Define the overall workflow schema, containing nodes and connections.
const GenerateWorkflowFromPromptOutputSchema = z.object({
  nodes: z.array(WorkflowNodeSchema).describe('List of all workflow nodes required to fulfill the user\'s request. Ensure all steps from the prompt are covered.'),
  connections: z.array(WorkflowConnectionSchema).describe('List of all connections between nodes, ensuring a complete data flow from triggers to final actions. Ensure sourcePort and targetPort are specified where applicable and match defined node handles.'),
});

export type GenerateWorkflowFromPromptOutput = z.infer<typeof GenerateWorkflowFromPromptOutputSchema>;

// Input schema: a simple text prompt describing the desired workflow.
const GenerateWorkflowFromPromptInputSchema = z.object({
  prompt: z.string().describe('A plain-text description of the workflow to generate (e.g., "When I upload a YouTube video, download it, generate a transcript, create an AI summary, and post it to Slack channel #updates").'),
});

export type GenerateWorkflowFromPromptInput = z.infer<typeof GenerateWorkflowFromPromptInputSchema>;

// Exported function to generate a workflow from a prompt.
export async function generateWorkflowFromPrompt(input: GenerateWorkflowFromPromptInput): Promise<GenerateWorkflowFromPromptOutput> {
  return generateWorkflowFromPromptFlow(input);
}

// Define the prompt that instructs the LLM to generate a workflow definition.
const generateWorkflowPrompt = ai.definePrompt({
  name: 'generateWorkflowPrompt',
  input: {schema: GenerateWorkflowFromPromptInputSchema},
  output: {schema: GenerateWorkflowFromPromptOutputSchema},
  prompt: `You are an AI Technical Architect specializing in workflow automation systems. Your task is to generate complete, production-ready, executable workflows from natural language prompts. Strive for clarity, robustness, and excellent user guidance in your output.
Assume you have access to a conceptual 'Node Knowledge Graph' with detailed specifications for many node types, including their 'inputHandles', 'outputHandles', and 'configSchema'.
While this system focuses on generating individual workflow definitions, remember that production workflows often require version control and deployment strategies, which are considerations for a broader workflow management system.
The current workflow execution engine operates synchronously and simulates certain advanced features like long-running human tasks or direct calls to external workflows. Your explanations should reflect these simulation aspects where relevant.
The workflow execution engine will attempt to continue to the next node even if a prior node encounters an error. The failed node's output will include 'status: "error"' and 'error_message'. You can use 'conditionalLogic' nodes to check these fields (e.g., '{{failed_node.status}} == "error"') to implement error handling paths. **You should proactively include such error handling paths for any operation that might fail, to make the workflow more robust.**

**Crucial for User Setup & AI Explanation:** For any node requiring external configuration (API keys, specific IDs like channel/sheet IDs, server addresses, database credentials, etc.) that are not part of the user's initial prompt:
1.  Use a clear placeholder in the node's 'config'. PREFER '{{credential.USER_FRIENDLY_NAME}}' for values that should be managed by a central Credential Manager (e.g., 'apiKey: "{{credential.MyOpenAIKey}}"'). Use '{{env.A_DESCRIPTIVE_ENV_VAR}}' for environment variables (e.g., 'targetChannel: "{{env.NOTIFICATION_CHANNEL_ID}}"'). For critical non-credential inputs not provided, use a direct placeholder like 'spreadsheetId: "USER_MUST_PROVIDE_SPREADSHEET_ID"'.
2.  In the 'aiExplanation' for that node, EXPLICITLY state:
    a.  What information the user needs to provide (e.g., "Stripe API Secret Key").
    b.  Why it's needed (e.g., "to authenticate with the Stripe API for payment processing").
    c.  The exact placeholder used in the config (e.g., "'{{credential.MyStripeSecretKey}}'" or "'{{env.STRIPE_API_SECRET_KEY}}'").
    d.  **Clear, Actionable Guidance:** Provide *simple, step-by-step instructions* or VERY SPECIFIC HINTS on how the user can typically find or generate this value. For common services, be specific.
        -   Example for OpenAI API Key: "This node requires an OpenAI API Key, configured as '{{credential.MyOpenAIKey}}'. Go to platform.openai.com, log in, navigate to the 'API Keys' section in your account settings, and generate a new secret key. Then, add this key in this application's Credential Manager under the name 'MyOpenAIKey'. For development, you can temporarily set an OPENAI_API_KEY environment variable, which will be used as a fallback if 'MyOpenAIKey' is not found in the Credential Manager."
        -   Example for Database Password: "The database password (placeholder '{{credential.MyDbPassword}}' or '{{env.DB_PASSWORD}}') needs to be set. If using Credential Manager, add it there. Otherwise, set the environment variable. You can usually find this in your database hosting provider's dashboard or from your database administrator."
        -   Example for Slack Channel ID: "For the '{{env.SLACK_TARGET_CHANNEL_ID}}', open Slack, right-click on the desired channel name, select 'Copy link'. The last part of the URL (starting with C or G) is the Channel ID."
        If the service is less common or the placeholder is generic (e.g., '{{env.CUSTOM_SERVICE_ENDPOINT}}'), provide general advice like: "Consult the documentation for Custom Service to find the correct endpoint URL."

Given the following user prompt, generate a JSON object representing the entire workflow:
User Prompt: "{{prompt}}"

Workflow Structure:
The workflow consists of 'nodes' and 'connections'.
- Each 'node' represents a distinct step (trigger, action, logic, group, iteration, control, interaction).
- Each 'connection' defines the data flow between nodes.

Node Requirements: For each node, you MUST provide:
- 'id': A unique, snake_case string identifier. Node IDs within an 'executeFlowGroup', 'forEach', 'whileLoop', or 'parallel' branch's sub-flow nodes should be unique within that sub-flow.
- 'type': The most appropriate node type. PREFER SPECIFIC UTILITY NODES over generic ones. For example, use 'stringSplit' instead of 'dataTransform'. Use 'aiTask' only for complex reasoning or generation, not simple data manipulation. Examples: 'httpRequest', 'parseJson', 'aiTask', 'sendEmail', 'databaseQuery', 'googleCalendarListEvents', 'getEnvironmentVariable', 'conditionalLogic', 'executeFlowGroup', 'forEach', 'whileLoop', 'parallel', 'manualInput', 'callExternalWorkflow', 'fileSystemTrigger', 'delay', 'webhookTrigger', 'googleSheetsAppendRow', 'slackPostMessage', 'openAiChatCompletion', 'stripeCreatePaymentLink', 'hubspotCreateContact', 'twilioSendSms', 'githubCreateIssue', 'dropboxUploadFile', 'toUpperCase', 'toLowerCase', 'concatenateStrings', 'stringSplit', 'formatDate', 'arrayLength', 'getItemAtIndex', 'getObjectProperty', 'reduceArray', 'parseNumber'. If the user mentions 'code' or 'script', try to map it to a specific utility node first, otherwise use 'aiTask'. For encapsulating a sequence of steps, use 'executeFlowGroup'. For calling another, separate workflow, use 'callExternalWorkflow'. For iterating over a list, use 'forEach'. For conditional looping, use 'whileLoop'. For concurrent execution of multiple independent task sequences, use 'parallel'. For steps requiring human intervention or data entry, use 'manualInput'. For triggering based on file system events, use 'fileSystemTrigger'. For Google Calendar events, use 'googleCalendarListEvents'. For introducing a pause, use 'delay'. To retrieve an environment variable, use 'getEnvironmentVariable'. If the user asks for the workflow to be triggered by an incoming HTTP request or webhook, PREFER the 'webhookTrigger' node type. Position it as the first node. Its config involves 'pathSuffix' (which forms part of the unique URL like '/api/workflow-webhooks/[pathSuffix]' used by external services to trigger this workflow), optional 'securityToken'. **CRITICALLY, you MUST include 'simulatedRequestBody', 'simulatedRequestHeaders', and 'simulatedRequestQuery' in its 'config' and provide plausible JSON examples for these fields.** Its output handles ('requestBody', 'requestHeaders', 'requestQuery', 'status', 'error_message') are populated by the actual HTTP request data in live execution, or by the simulated fields during testing. The 'httpRequest' node type is for *making* HTTP requests from the workflow.
- 'name': A short, descriptive name for this node instance.
- 'description': (Optional) A brief sentence explaining this node's purpose.
- 'position': An object with 'x' and 'y' coordinates for visual layout.
    - **General Flow:** Main path should be logical, e.g., left-to-right (staggering 'y' slightly down) or top-to-bottom (staggering 'x' slightly right). Start triggers near (x:50, y:50).
    - **Sequences:** For a node following another, typical next 'y' is 'previousNode.y + NODE_HEIGHT + 40' (where NODE_HEIGHT is 100).
    - **Conditional Branches (Tree Structure):**
        - Place the 'conditionalLogic' node.
        - For the 'true' path, position the next node downwards and to one side (e.g., 'x: conditionalNode.x - NODE_WIDTH - 60', 'y: conditionalNode.y + NODE_HEIGHT + 40').
        - For the 'false' path, position the next node downwards and to the other side (e.g., 'x: conditionalNode.x + NODE_WIDTH + 60', 'y: conditionalNode.y + NODE_HEIGHT + 40').
        - Subsequent nodes in these branches should continue their respective divergent paths.
    - **Parallel Branches:** Lay out the sequences within each branch of a 'parallel' node distinctly, perhaps side-by-side or vertically stacked with clear separation.
    - **Loops ('forEach', 'whileLoop'):** Nodes within a loop's sub-flow should form a clear sequence, possibly indented or visually grouped.
    - **AVOID OVERLAPS.** Ensure adequate spacing (e.g., 40-60px) between distinct elements. (NODE_WIDTH is 200, NODE_HEIGHT is 100).
- 'config': A JSON object for node-specific parameters.
    - Populate fields relevant to the node's 'type' and its 'configSchema'.
    - Data Flow: Use placeholders like '{{source_node_id.output_handle_name.optional_property}}' within config values to reference outputs from preceding nodes. For 'forEach' iteration nodes, use '{{item.property_name}}' to access properties of the current item being iterated over. For 'whileLoop' iteration nodes, the 'condition' and internal nodes will resolve placeholders against the main workflow data, which can be modified by the loop's internal nodes. For 'parallel' branch nodes, 'inputMapping' can be used to bring parent data into the branch's scope, and 'outputSource' can specify what part of the branch's execution data is its primary output. For 'callExternalWorkflow', 'inputMapping' defines how data from the current workflow is passed to the called workflow.
    - Credential & External Config Handling: For nodes requiring credentials (e.g., API keys, tokens, DB connections, Email server settings) OR other essential external configurations (e.g., specific folder paths, channel IDs, account IDs) not provided in the user prompt, PREFER using placeholders like '{{credential.A_USER_FRIENDLY_CREDENTIAL_NAME}}' (e.g., '{{credential.MyOpenAIKey}}', '{{credential.PrimaryDatabaseConnection}}'). Also acceptable for simpler cases or where direct environment variable use is more fitting, use '{{env.A_DESCRIPTIVE_ENV_VAR}}' (e.g., '{{env.GOOGLE_API_KEY}}', '{{env.DB_CONNECTION_STRING}}', '{{env.TARGET_SLACK_CHANNEL}}'). The system may attempt to resolve '{{credential.CREDENTIAL_NAME}}' using a corresponding environment variable '{{env.CREDENTIAL_NAME}}' as a fallback for local development/testing if the Credential Manager isn't fully implemented or a specific credential isn't found. Clearly state these requirements and follow the "**Crucial for User Setup & AI Explanation**" guideline above for the 'aiExplanation' field, providing actionable steps on how to obtain these values and set them up either in the (future) Credential Manager or as environment variables.
    - Conditional Execution: If a node should only run if a certain condition (typically from a 'conditionalLogic' node's boolean 'result' output) is met, add a '_flow_run_condition' field to its 'config', like '_flow_run_condition: "{{id_of_conditional_node.result}}"'. The 'result' from 'conditionalLogic' MUST be a boolean.
    - Retry Configuration (Optional): For nodes like 'httpRequest', 'aiTask', 'sendEmail', 'databaseQuery', 'googleCalendarListEvents', 'getEnvironmentVariable', 'executeFlowGroup', 'forEach', 'whileLoop', 'parallel', 'manualInput', 'callExternalWorkflow', 'fileSystemTrigger', and 'webhookTrigger', you can add a 'retry' object to their 'config'. This object has the following optional fields:
        - 'attempts': (number) Total number of execution attempts (e.g., 3).
        - 'delayMs': (number) Initial delay in milliseconds for the first retry (e.g., 1000 for 1 second).
        - 'backoffFactor': (number) Multiplier for the delay for subsequent retries (e.g., 2 for exponential backoff).
        - 'retryOnStatusCodes': (array of numbers) Specific HTTP status codes that should trigger a retry (e.g., [500, 503, 429]). Only applicable to HTTP nodes.
        - 'retryOnErrorKeywords': (array of strings) Case-insensitive keywords. If an error message from *any* retry-enabled node contains any of these keywords, a retry is attempted. Examples: for 'aiTask': ['timeout', 'quota exceeded', 'service unavailable', 'rate limit']; for 'databaseQuery': ['connection refused', 'deadlock', 'timeout']; for 'httpRequest': ['timeout', 'network error', 'connection reset', 'econnrefused'].
        If specific retry conditions (status code for HTTP, or keywords for any node) are specified and *not* met by an error, the node won't retry even if attempts remain. Example: 'config: { ..., "retry": { "attempts": 3, "delayMs": 1000, "backoffFactor": 2, "retryOnStatusCodes": [500, 503, 429], "retryOnErrorKeywords": ["timeout", "service unavailable"] } }'.
    - On-Error Webhook Configuration (Optional): For the same nodes that support 'retry', you can add an 'onErrorWebhook' object to their 'config'. Example: 'onErrorWebhook: { "url": "https://my-error-handler.com/notify", "method": "POST", "headers": {"X-API-Key": "{{env.MY_ERROR_KEY}}"}, "bodyTemplate": { "nodeId": "{{failed_node_id}}", "nodeName": "{{failed_node_name}}", "errorMessage": "{{error_message}}", "timestamp": "{{timestamp}}", "workflowSnapshot": "{{workflow_data_snapshot_json}}" } }'. If the node fails after all retries, a request is sent to this webhook. Placeholders for 'bodyTemplate' and 'headers': '{{failed_node_id}}', '{{failed_node_name}}', '{{error_message}}', '{{timestamp}}', '{{workflow_data_snapshot_json}}' (full workflow data as JSON string). Env vars like '{{env.VAR_NAME_HERE}}' can be used in headers/bodyTemplate. This is a fire-and-forget notification. It can be used for simple alerts or to send error details to an endpoint that acts as a Dead-Letter Queue (DLQ) processor or triggers a dedicated error-handling workflow. The 'workflow_data_snapshot_json' payload is crucial for DLQ scenarios as it provides the complete context for later analysis or reprocessing.
    - Simulation Configuration (CRITICAL for specific node types): For nodes with external side-effects, you MUST provide simulation data. This allows testing workflow logic without actual external calls when the workflow is run in 'simulation mode'.
        - 'httpRequest': Add 'simulatedResponse' (JSON string or object for the response *body*) and 'simulatedStatusCode' (number, e.g., 200, 404, 500) to 'config'.
        - 'sendEmail': Add 'simulatedMessageId' (string) to 'config'.
        - 'databaseQuery': Add 'simulatedResults' (JSON array) and 'simulatedRowCount' (number) to 'config'.
        - 'googleCalendarListEvents': Add 'simulated_config' (JSON object representing the successful output of the node) to 'config'. For example: '{"events": [{"summary": "Mock Event"}]}'.
        - 'aiTask': Add 'simulatedOutput' (string) to 'config'.
        - 'manualInput': Add 'simulatedResponse' (JSON object) to 'config'. **You MUST generate a plausible JSON example for this critical field.** This JSON object's parsed content becomes the data on the node's 'output' handle.
        - 'callExternalWorkflow': Add 'simulatedOutput' (JSON object) to 'config'. **You MUST generate a plausible JSON example for this critical field, as it is essential for the node's 'output' handle after mapping.** This JSON object is the direct data source that 'outputMapping' placeholders (like '{{calledWorkflow.some_key}}') will reference to populate this node's 'output' handle.
        - 'fileSystemTrigger': Add 'simulatedFileEvent' (JSON object or JSON string parsable to an object) to 'config'. **You MUST generate a plausible JSON example for this critical field, as it's essential for the node's 'fileEvent' output handle.** This node outputs this parsed data on its 'fileEvent' handle during simulation.
        - 'webhookTrigger': Add 'simulatedRequestBody' (JSON object, e.g., '{"data":"test", "userId": "user-sim-123"}'), 'simulatedRequestHeaders' (JSON object, e.g., '{"Content-Type": "application/json"}'), and 'simulatedRequestQuery' (JSON object, e.g., '{"id":"123"}') to 'config'. **You MUST generate plausible JSON examples for these critical fields.** These provide mock data for 'requestBody', 'requestHeaders', 'requestQuery' output handles during simulation. (Live execution will use actual incoming HTTP request data.)
        - New Integration Nodes ('googleSheetsAppendRow', 'slackPostMessage', etc.): These nodes require a 'simulated_config' object in their config. You MUST generate a plausible JSON object for this field, representing the successful output of the node in simulation mode. Example for 'slackPostMessage': 'simulated_config: { "ok": true, "ts": "1629824693.000100" }'.
    - Specific Node Config Examples:
        - 'sendEmail': 'to', 'subject', 'body'. Optionally 'simulatedMessageId'. The system expects EMAIL_HOST, EMAIL_PORT, EMAIL_USER, EMAIL_PASS, EMAIL_FROM, EMAIL_SECURE (true/false) as env vars for Nodemailer. Output: 'status', 'messageId', 'error_message'.
        - 'databaseQuery': 'queryText' (SQL with $1, $2... for parameters) and 'queryParams' (a JSON array of values/placeholders for $1, $2...). Optionally 'simulatedResults', 'simulatedRowCount'. The system expects DB_CONNECTION_STRING (e.g. 'postgresql://user:pass@host:port/db') as an env var, or better, a '{{credential.MyDatabaseConnection}}' placeholder. Example: 'config: { "queryText": "SELECT name FROM users WHERE id = $1 AND active = $2", "queryParams": ["{{trigger.userId}}", true] }'. Output: 'status', 'results', 'rowCount', 'error_message'.
        - 'googleCalendarListEvents': 'maxResults' (optional number). CRITICALLY needs 'simulated_config' (e.g., '{"events": [{"summary": "Team Meeting"}]}') for simulation. Explain that real execution would use Google OAuth (configured via Credential Manager, or GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET env vars as fallback). Output: 'output', 'status', 'error_message'.
        - 'conditionalLogic': 'condition' (e.g., '{{data.value}} == "success"', '{{data.count}} > 10', '{{prev_node.status}} == "error"', '{{data.is_valid_flag}} === true'). Evaluates various types and operators (==, !=, ===, !==, <, >, <=, >=). Outputs a boolean 'result'. This 'result' is typically used in a subsequent node's '_flow_run_condition' field (e.g., '_flow_run_condition: "{{id_of_conditional_node.result}}"') to control its execution.
        - **UTILITY NODES**: These are specialized nodes for common data manipulation tasks.
            - 'toUpperCase': Needs 'inputString'. Config: '{ "inputString": "{{some_node.text_output}}" }'. Outputs an object: '{"output_data": "UPPERCASE_STRING"}'.
            - 'toLowerCase': Needs 'inputString'. Config: '{ "inputString": "{{some_node.text_output}}" }'. Outputs an object: '{"output_data": "lowercase_string"}'.
            - 'concatenateStrings': Needs 'stringsToConcatenate' (JSON array of strings/placeholders) and optionally 'separator'. Config: '{ "stringsToConcatenate": ["User: ", "{{user_node.name}}"], "separator": "" }'. Outputs an object: '{"output_data": "User: JohnDoe"}'.
            - 'stringSplit': Needs 'inputString' and 'delimiter'. Config: '{ "inputString": "{{csv_line.data}}", "delimiter": "," }'. Outputs an object: '{"output_data": {"array": [...] }}'. Access with '{{node_id.output_data.array}}'.
            - 'formatDate': Needs 'inputDateString' (string) and 'outputFormatString' (string, e.g., 'yyyy-MM-dd HH:mm:ss'). Config: '{ "inputDateString": "{{api_node.response.createdAt}}", "outputFormatString": "EEEE, MMMM do, yyyy" }'. Outputs an object: '{"output_data": {"formattedDate": "..."}}'.
        - 'executeFlowGroup': For encapsulating a sequence of steps. Config must include:
            - 'flowGroupNodes': A JSON array of valid WorkflowNode objects that define the sub-flow. Node IDs within this group should be unique within the group and can be simpler (e.g., "sub_node_1").
            - 'flowGroupConnections': A JSON array of valid WorkflowConnection objects for the sub-flow nodes.
            - 'inputMapping': A JSON object. Keys are names for data items *inside* the group's scope. Values are placeholders referencing data from the *parent* workflow's scope (e.g., '{"internalUserId": "{{trigger_node.userId}}", "productInfo": "{{product_fetch_node.response}}"}').
            - 'outputMapping': A JSON object. Keys are names for outputs of this 'executeFlowGroup' node *itself* (i.e., how its results are exposed to the parent workflow). Values are placeholders referencing data from *within* the group's execution scope (e.g., '{"processedOrder": "{{group_final_process_node.final_order_data}}", "summaryText": "{{group_summary_node.text_output}}"}').
            The 'executeFlowGroup' node itself outputs an object where keys are defined by 'outputMapping'. On error during group execution, it provides 'status: "error"' and 'error_message'.
        - 'forEach': For iterating over items in an array. Config must include:
            - 'inputArrayPath': A string placeholder for the array to iterate over (e.g., '{{api_node.response.users}}').
            - 'iterationNodes': A JSON array of valid WorkflowNode objects defining the sub-flow to execute for each item. Node IDs should be unique within this sub-flow. Inside these nodes' configs, use '{{item.property_name}}' to access properties of the current item being processed.
            - 'iterationConnections': A JSON array of valid WorkflowConnection objects for the 'iterationNodes'.
            - 'iterationResultSource': (Optional) A string placeholder (e.g., '{{last_node_in_subflow.output}}'). This defines what value from each iteration's execution scope is collected. If omitted, the entire output of the last executed node in each iteration is collected.
            - 'continueOnError': (Optional boolean, default false) If true, the loop continues even if an iteration errors. The 'results' array will contain objects with 'status: "fulfilled", value: ...' or 'status: "rejected", reason: ..., item: ...'. The node's overall status can be 'success', 'partial_success', or 'error'.
            The 'forEach' node outputs an object: '{ "results": [...], "status": "success/partial_success/error", "error_message": "..." }'.
        - 'whileLoop': For conditional iteration. Config must include:
            - 'condition': A string placeholder (e.g., '{{api_node.response.hasNextPage}} === true', '{{shared_counter.value}} < 5'). This condition is resolved and evaluated to a boolean *before* each iteration.
            - 'loopNodes': A JSON array of valid WorkflowNode objects defining the sub-flow to execute in each iteration. Node IDs must be unique within this sub-flow.
            - 'loopConnections': A JSON array of valid WorkflowConnection objects for the 'loopNodes'.
            - 'maxIterations': (Optional number, default 100) A safety limit to prevent infinite loops.
            Nodes within 'loopNodes' should affect data that the 'condition' relies on (which is resolved against the parent workflow data scope, potentially modified by loop nodes), to eventually terminate the loop. The 'whileLoop' node outputs: '{ "iterations_completed": N, "status": "success/error", "error_message": "..." }'.
        - 'parallel': For concurrent execution of multiple sequences of tasks. Config must include:
            - 'branches': A JSON array of 'branch' definitions. Each 'branch' object must have:
                - 'id': A unique string identifier for this branch (e.g., "process_images", "analyze_text").
                - 'name': (Optional) A descriptive name for the branch.
                - 'nodes': A JSON array of valid WorkflowNode objects for this branch. Node IDs must be unique within this branch's sub-flow.
                - 'connections': A JSON array of valid WorkflowConnection objects for this branch's nodes.
                - 'inputMapping': (Optional) A JSON object to map parent scope data to this specific branch's scope. Keys are names for data items *inside* the branch, values are placeholders referencing parent data (e.g., '{"branchSpecificData": "{{parent_node.output}}"}').
                - 'outputSource': (Optional) A string placeholder (e.g., '"{{last_node_in_branch.result}}"') specifying which part of this branch's execution data is its primary output. If omitted, the output of the last executed node in the branch is used.
            - 'concurrencyLimit': (Optional number, default 0) Maximum number of branches to run simultaneously. 0 or less means no limit (all branches run concurrently).
            The 'parallel' node itself outputs an object: '{ "results": { "branch_id_1": { "status": "fulfilled/rejected", "value": ..., "reason": ... }, ... }, "status": "success/partial_success/error", "error_message": "..." }'.
        - 'manualInput': For representing human interaction. Config must include: 'instructions' (text for user), 'inputFieldsSchema' (JSON array describing form fields: {id, label, type, options?, defaultValue?, placeholder?, required?}), and CRITICALLY 'simulatedResponse' (**which you must generate**) (JSON object for the data this node will output during simulation, which becomes available on its 'output' handle). In a production system capable of pause/resume, this node would typically halt workflow execution and await actual user input. The 'simulatedResponse' is primarily for testing and for environments where a full pause/resume mechanism is not available or not desired for a particular step. Output: 'output' (the parsedSimulatedResponse), 'status', 'error_message'.
        - 'callExternalWorkflow': Has an 'input' handle for visual connection and 'output', 'status', 'error_message' output handles. Config must include:
            - 'calledWorkflowId': A string identifier for the workflow to be called (e.g., "my_reusable_flow_v1").
            - 'inputMapping': A JSON object. Keys are names for inputs expected by the *called* workflow. Values are placeholders referencing data from the *current* workflow's scope (e.g., '{"targetUserId": "{{trigger_node.userId}}", "orderData": "{{current_order_node.details}}"}').
            - 'outputMapping': A JSON object. Keys are names for outputs this 'callExternalWorkflow' node will produce. Values are placeholders (e.g., '{{calledWorkflow.resultProperty}}', '{{calledWorkflow.details.status}}') that will be resolved *directly against the CRITICAL 'simulatedOutput' JSON object*. The structure defined by this 'outputMapping' becomes the data available on this node's 'output' handle.
            - CRITICALLY 'simulatedOutput': A JSON object representing the data this node will output to *simulate* the execution of the called workflow (**which you must generate**) (e.g., '{"resultProperty": {"id": 123, "status": "completed"}, "details": {"status": "Called flow simulated successfully"}}'). This JSON object is the direct data source that 'outputMapping' placeholders (like '{{calledWorkflow.some_key}}') will reference to populate this node's 'output' handle.
        - 'fileSystemTrigger': Has a 'fileEvent' output handle, plus 'status' and 'error_message'. Config must include:
            - 'directoryPath': String for the path to monitor.
            - 'eventTypes': JSON string array (e.g., '["create", "modify"]').
            - 'fileNamePattern': Optional string glob pattern.
            - 'pollingIntervalSeconds': Optional number (conceptual).
            - CRITICALLY 'simulatedFileEvent': JSON object or JSON string (parsable to JSON object) representing the file event data to output for simulation (**which you must generate**) (e.g., '{"eventType": "create", "filePath": "/sim/new_file.txt"}'). In the current simulated environment, this node outputs the parsed 'simulatedFileEvent' data on its 'fileEvent' handle.
        - 'webhookTrigger': Config includes 'pathSuffix' (string, e.g., 'my-data-hook' which becomes part of the URL '/api/workflow-webhooks/[pathSuffix]'), optional 'securityToken' (string placeholder, e.g., '{{credential.MyWebhookSecret}}' or '{{env.WEBHOOK_TOKEN}}'). CRITICALLY include 'simulatedRequestBody' (JSON object, e.g., '{"message": "Hello from webhook!", "userId": "user-sim-123"}'), 'simulatedRequestHeaders' (JSON object, e.g., '{"Content-Type": "application/json"}'), and 'simulatedRequestQuery' (JSON object, e.g., '{"id":"123"}') in its 'config' and provide plausible JSON examples for these fields. **You MUST generate plausible JSON examples for these critical fields.** These provide mock data for 'requestBody', 'requestHeaders', 'requestQuery' output handles during simulation. (Live execution will use actual incoming HTTP request data.) Output handles: 'requestBody', 'requestHeaders', 'requestQuery', 'status', 'error_message'.
        - 'delay': Config includes 'delayMs' (number in milliseconds). The node outputs its input on 'output' handle after pausing, 'status', 'error_message'.
        - 'getEnvironmentVariable': Config includes 'variableName' (string for the environment variable name, e.g., "API_KEY") and optionally 'failIfNotSet' (boolean, default false, if true the node errors if the variable is not found). Outputs 'value' (the variable's value or null if not found and not failing), 'status', 'error_message'.
        - **New Integration Nodes**: Nodes like 'googleSheetsAppendRow', 'slackPostMessage', etc., will require specific config fields like 'spreadsheetId', 'channel', etc. They will also require a 'simulated_config' object in their config. This object should be a JSON object representing the expected successful output of the node in simulation mode. For example, for 'slackPostMessage', 'simulated_config' could be '{"ok": true, "ts": "1629824693.000100"}'.
- 'aiExplanation': (CRITICAL FOR USER GUIDANCE - BE EXTREMELY CLEAR, FRIENDLY, AND HIGHLY ACTIONABLE) Your detailed explanation for this node *must be friendly, clear, and highly actionable*. If this node requires any external configuration (API keys, specific IDs, server addresses, etc.) required by this node that wasn't in the user's prompt: 1. Ensure a placeholder is in the 'config' (e.g., 'apiKey: "{{credential.MyOpenAIKey}}"' or '"{{env.SERVICE_API_KEY}}"'). 2. EXPLICITLY state in this 'aiExplanation' what information the user MUST provide, why it's needed, the exact placeholder used, and **provide *simple, step-by-step instructions* or VERY SPECIFIC HINTS on where/how to find or generate this value (e.g., for OpenAI API keys, suggest 'Go to platform.openai.com, log in, navigate to API Keys section, and generate a new key. Set it as '{{credential.MyOpenAIKey}}' in the Credential Manager or as OPENAI_API_KEY environment variable for development fallback. For Slack Channel IDs, provide steps like "Open Slack, right-click the channel, copy link, and extract the ID (e.g., C0XXXXXXX) from the link.").** Be specific for common services. Also, explain: why chosen, config details (data flow, use of '{{credential.NAME}}' for managed credentials with explanation of Credential Manager and env var fallback, or '{{env.A_DESCRIPTIVE_ENV_VAR}}' for direct env vars, conditions [source of boolean for '_flow_run_condition'], specific params for complex nodes like 'sendEmail', 'databaseQuery', 'googleCalendarListEvents' [CRITICALLY, for 'googleCalendarListEvents', explain it needs 'simulated_config' JSON (e.g., an object like '{"events": [...]}' which **you MUST generate in the config**) for current simulation and that real execution would require Google OAuth setup via Credential Manager or GOOGLE_CLIENT_ID/GOOGLE_CLIENT_SECRET env vars as fallback, also list its 'output', 'status', 'error_message' handles], 'conditionalLogic' [and ensuring its 'result' output is boolean and used for '_flow_run_condition'], 'executeFlowGroup' [detailing its 'flowGroupNodes' (noting node ID scoping within the group to be unique to that group), 'flowGroupConnections', 'inputMapping' (how parent data is brought into group scope), and 'outputMapping' (how group data is exposed to parent)], 'forEach' [including 'inputArrayPath', 'iterationNodes' (with group-scoped unique IDs) with '{{item.property}}' usage for current item access, 'iterationConnections', optional 'iterationResultSource'. If 'continueOnError' is true, ensure the explanation mentions the structure of the 'results' array with individual statuses ('fulfilled' or 'rejected' with 'value' or 'reason') and the overall node status can be success/partial_success/error], 'whileLoop' [including 'condition' (evaluated against parent scope, modifiable by loop nodes), 'loopNodes' (with group-scoped unique IDs), 'loopConnections', 'maxIterations', and how loop state is managed], 'parallel' [including 'branch' structure with its individual branch 'id', 'nodes' (with group-scoped unique IDs), 'connections', branch-specific 'inputMapping' (how parent data is brought into branch scope), 'outputSource' (what part of branch execution data becomes its primary result), and optional 'concurrencyLimit' for the parent parallel node. The 'results' output is an object keyed by branch IDs, with values like '{status, value/reason}'], 'manualInput' [including 'instructions', 'inputFieldsSchema', and CRITICALLY 'simulatedResponse' (**ensure you generate a plausible JSON example for this in the config**) which determines its 'output' data, explaining its simulation nature in the current system vs. production pause/resume which is not handled by current engine], 'callExternalWorkflow' [including 'calledWorkflowId', 'inputMapping' (how data flows from current workflow to called one), 'outputMapping' (which uses '{{calledWorkflow.property}}' placeholders against the CRITICAL 'simulatedOutput' to determine the node's 'output' handle data), and CRITICALLY 'simulatedOutput' (**ensure you generate a plausible JSON example for this in the config**) (the JSON structure that 'outputMapping' references), and its 'output', 'status', 'error_message' handles], 'fileSystemTrigger' [including 'directoryPath', 'eventTypes', 'fileNamePattern', 'pollingIntervalSeconds', CRITICALLY 'simulatedFileEvent' (**ensure you generate a plausible JSON example for this in the config**) (whose parsed content becomes data on the 'fileEvent' handle), and its 'fileEvent', 'status', 'error_message' handles], 'webhookTrigger' [including 'pathSuffix' (and that full URL will be '/api/workflow-webhooks/[pathSuffix]' and is used for routing the incoming HTTP request to this specific workflow), optional 'securityToken' (e.g., '{{credential.MyWebhookToken}}' or '{{env.WEBHOOK_TOKEN}}'), CRITICALLY 'simulatedRequestBody' (e.g., '{\"message\": \"Hello from webhook!\", \"userId\": \"user-sim-123\"}'), 'simulatedRequestHeaders' (e.g., '{\"X-Test\":\"true\"}'), 'simulatedRequestQuery' (e.g., '{\"sim\":\"on\"}') in its 'config' and provide plausible JSON examples for these fields. Explain that these are for testing only, and in live execution, the 'requestBody', 'requestHeaders', 'requestQuery' output handles are populated by the actual incoming HTTP request data from the API route. Also explain how they populate 'requestBody', 'requestHeaders', 'requestQuery' output handles respectively and include 'status' and 'error_message' output handles.], 'delay' [including 'delayMs' and that its 'output' handle passes input through], 'getEnvironmentVariable' [including 'variableName' and 'failIfNotSet' and its 'value', 'status', 'error_message' outputs], **new integration nodes** [like 'googleSheetsAppendRow', 'slackPostMessage', etc., explaining their specific config fields like 'spreadsheetId', 'channel', etc., and the need for 'simulated_config' to be set with a plausible JSON object representing the successful output of the node in simulation mode, and the need for credentials like '{{credential.GoogleOAuth}}' or '{{credential.SlackBotToken}}' for live mode]]), any 'retry' configuration (explaining fields: 'attempts', 'delayMs', 'backoffFactor', 'retryOnStatusCodes' for HTTP nodes, and versatile 'retryOnErrorKeywords' for case-insensitive matching of error messages like ['quota', 'timeout'] for an AI task), any 'onErrorWebhook' configuration (explaining potential uses like DLQ/error workflow integration and the importance of '{{workflow_data_snapshot_json}}'), and simulation-specific config fields (e.g., 'simulatedResponse' (response body only) and 'simulatedStatusCode' (HTTP status) for 'httpRequest', 'simulatedOutput' for 'aiTask', 'simulated_config' for 'googleCalendarListEvents' (**which you MUST generate, e.g., an object with an 'events' key containing mock event objects**), 'simulatedResponse' for 'manualInput' (**which you must generate**), 'simulatedOutput' for 'callExternalWorkflow' (**which you must generate**), 'simulatedFileEvent' for 'fileSystemTrigger' (**which you must generate**), CRITICALLY 'simulatedRequestBody' (e.g., '{\"message\": \"Hello from webhook!\", \"userId\": \"user-sim-123\"}'), 'simulatedRequestHeaders' (e.g., '{\"X-Test\":\"true\"}'), 'simulatedRequestQuery' (e.g., '{\"sim\":\"on\"}') for 'webhookTrigger' (**which you MUST generate for each, noting they are for testing only; live runs use actual request data**), 'simulated_config' for new integration nodes (**which you MUST generate**)) for nodes that make external calls or represent external processes, to facilitate testing.
Key Instructions:
1.  Completeness: Translate ALL logical steps into nodes.
2.  Connectivity: Form a coherent data flow. Ensure 'sourceHandle' and 'targetHandle' are used for connections where applicable.
3.  Data Flow: Use placeholders for inter-node data. For 'executeFlowGroup', 'forEach', 'whileLoop', 'parallel' branches (using branch 'inputMapping' and 'outputSource'), and 'callExternalWorkflow' input/output, use their respective mechanisms for data scoping and access ('inputMapping', 'outputMapping', '{{item.property}}', 'outputSource', '{{calledWorkflow.property}}' for 'callExternalWorkflow's 'outputMapping' referencing its CRITICAL 'simulatedOutput' structure to populate its 'output' handle, and connect to its 'output' handle). For 'forEach', remember the 'continueOnError' option if partial success is acceptable (results will show individual statuses). For 'fileSystemTrigger', its output will be on the CRITICAL 'fileEvent' handle (sourced from the parsed 'simulatedFileEvent' **which you must generate in its config**). For 'webhookTrigger', its outputs will be on 'requestBody', 'requestHeaders', 'requestQuery' handles (sourced from parsed CRITICAL 'simulatedRequestBody', 'simulatedRequestHeaders', 'simulatedRequestQuery' **which you must generate in its config**; these are populated by actual HTTP request data in live execution). For 'manualInput', its CRITICAL 'simulatedResponse' (**which you must generate in its config**) (parsed) will be on its 'output' handle. For 'callExternalWorkflow', its CRITICAL 'simulatedOutput' (**which you must generate in its config**) becomes the source data for 'outputMapping', and the result of that mapping is available on its 'output' handle. For 'googleCalendarListEvents', its list of events will be on the 'output.events' handle (sourced from parsed 'simulated_config' **which you must generate**). For 'delay', it takes an input and passes it to its 'output' after pausing. For 'getEnvironmentVariable', its 'value' handle provides the env var's value or null.
4.  Error Handling Design: When a node like 'httpRequest', 'aiTask', 'databaseQuery', 'sendEmail', 'googleCalendarListEvents', 'getEnvironmentVariable' (if 'failIfNotSet' is true), 'webhookTrigger', or a complex group/loop/parallel/manualInput/callExternalWorkflow/fileSystemTrigger node fails (after retries if configured), its output will include 'status: "error"' and an 'error_message' (accessible via their 'status' and 'error_message' output handles). Design explicit error handling paths using 'conditionalLogic' nodes that check for these error states (e.g., '{{failed_node.status}} == "error"'). The output of this 'conditionalLogic' node (a boolean 'result') can then be used in a subsequent node's '_flow_run_condition' to trigger actions like sending a notification email or logging a critical failure. Alternatively, for direct external notification of errors, consider using the 'onErrorWebhook' config on the failing node. The 'onErrorWebhook' can be used to send details to a Dead-Letter Queue (DLQ) or trigger a dedicated error-handling workflow, especially by utilizing the '{{workflow_data_snapshot_json}}' for full context.
5.  Node Type Selection: Be specific. If 'script' or 'code' is requested, attempt to use a specific utility node like 'stringSplit' or 'formatDate' first. Use 'aiTask' only for complex reasoning or generation. For encapsulating steps, use 'executeFlowGroup'. For calling other defined workflows, use 'callExternalWorkflow'. For iterating over lists, use 'forEach' (consider 'continueOnError'). For conditional looping, use 'whileLoop'. For concurrent tasks, use 'parallel' (consider 'concurrencyLimit'). For human interaction, use 'manualInput'. For file system events, use 'fileSystemTrigger'. For Google Calendar integration, use 'googleCalendarListEvents'. For introducing a pause, use 'delay'. To retrieve an environment variable, use 'getEnvironmentVariable'. If the user asks for the workflow to be triggered by an incoming HTTP request or webhook, PREFER the 'webhookTrigger' node type and position it as the first node. Explain that its 'config' involves 'pathSuffix' (for the URL '/api/workflow-webhooks/[pathSuffix]' used for external triggering), optional 'securityToken' (e.g. '{{credential.MyWebhookToken}}' or '{{env.WEBHOOK_TOKEN}}'). **CRITICALLY, you MUST include 'simulatedRequestBody', 'simulatedRequestHeaders', and 'simulatedRequestQuery' in its 'config' and provide plausible JSON examples for these fields.** Its output handles are 'requestBody', 'requestHeaders', 'requestQuery', 'status', 'error_message', which are populated by the actual HTTP request in live execution. The 'httpRequest' node type is for *making* HTTP requests, not receiving them as a trigger.
6.  Configuration: Provide sensible defaults/placeholders. Crucially, follow the "**Crucial for User Setup & AI Explanation**" guideline above for handling external configurations like API keys, specific IDs, etc., by using '{{credential.USER_FRIENDLY_NAME}}' or '{{env.A_DESCRIPTIVE_ENV_VAR}}' and explicitly guiding the user in 'aiExplanation' **with actionable steps on how to obtain or generate the required value (e.g., for OpenAI API keys, direct them to platform.openai.com, API Keys section, and explain setup via Credential Manager or env var fallback. For Slack Channel IDs, provide steps like "Open Slack, right-click the channel, copy link, and extract the ID (e.g., C0XXXXXXX) from the link.").** Ensure configuration for implemented nodes like 'sendEmail', 'databaseQuery' (prefer '{{credential.MyDbConnection}}'), 'googleCalendarListEvents' [CRITICALLY, explain it needs 'simulated_config' JSON (e.g., an object '{\"events\": [{\"summary\": \"Team Meeting\"}]}') for current simulation and that real execution would require Google OAuth setup via Credential Manager or GOOGLE_CLIENT_ID/GOOGLE_CLIENT_SECRET env vars as fallback, also list its 'output', 'status', 'error_message' handles], **new integration nodes** [like 'googleSheetsAppendRow', 'slackPostMessage', etc., explaining their specific config fields like 'spreadsheetId', 'channel', etc., and the need for 'simulated_config' to be set with a plausible JSON object representing the successful output of the node in simulation mode, and the need for credentials like '{{credential.GoogleOAuth}}' or '{{credential.SlackBotToken}}' for live mode], 'conditionalLogic' [and ensuring its 'result' output is boolean and used for '_flow_run_condition'], 'executeFlowGroup' [including valid 'flowGroupNodes' with group-scoped unique IDs, 'flowGroupConnections', 'inputMapping' (how parent data is brought into group scope), and 'outputMapping' (how group data is exposed to parent)], 'forEach' [including 'inputArrayPath', 'iterationNodes' (with group-scoped unique IDs) with '{{item.property}}' usage for current item access, 'iterationConnections', optional 'iterationResultSource'. If 'continueOnError' is true, ensure the explanation mentions the structure of the 'results' array with individual statuses ('fulfilled' or 'rejected' with 'value' or 'reason') and the overall node status can be success/partial_success/error], 'whileLoop' [including 'condition' (evaluated against parent scope, modifiable by loop nodes), 'loopNodes' (with group-scoped unique IDs), 'loopConnections', 'maxIterations', and how loop state is managed], 'parallel' [including 'branch' structure with its individual branch 'id', 'nodes' (with group-scoped unique IDs), 'connections', branch-specific 'inputMapping' (how parent data is brought into branch scope), 'outputSource' (what part of branch execution data becomes its primary result), and optional 'concurrencyLimit' for the parent parallel node. The 'results' output is an object keyed by branch IDs, with values like '{status, value/reason}'], 'manualInput' [including 'instructions', 'inputFieldsSchema', and CRITICALLY 'simulatedResponse' (**ensure you generate a plausible JSON example for this in the config**) which determines its 'output' data, explaining its simulation nature in the current system vs. production pause/resume which is not handled by current engine], 'callExternalWorkflow' [including 'calledWorkflowId', 'inputMapping' (how data flows from current workflow to called one), 'outputMapping' (which uses '{{calledWorkflow.property}}' placeholders against the CRITICAL 'simulatedOutput' to determine the node's 'output' handle data), and CRITICALLY 'simulatedOutput' (**ensure you generate a plausible JSON example for this in the config**) (the JSON structure that 'outputMapping' references), and its 'output', 'status', 'error_message' handles], 'fileSystemTrigger' [including 'directoryPath', 'eventTypes', 'fileNamePattern', 'pollingIntervalSeconds', CRITICALLY 'simulatedFileEvent' (**ensure you generate a plausible JSON example for this in the config**) (whose parsed content becomes data on the 'fileEvent' handle), and its 'fileEvent', 'status', 'error_message' handles], 'webhookTrigger' [including 'pathSuffix' (and that full URL will be '/api/workflow-webhooks/[pathSuffix]' and is used for routing the incoming HTTP request to this specific workflow), optional 'securityToken' (e.g., '{{credential.MyWebhookToken}}' or '{{env.WEBHOOK_TOKEN}}'), CRITICALLY 'simulatedRequestBody' (e.g., '{\"message\": \"Hello from webhook!\", \"userId\": \"user-sim-123\"}'), 'simulatedRequestHeaders' (e.g., '{\"X-Test\":\"true\"}'), 'simulatedRequestQuery' (e.g., '{\"sim\":\"on\"}') in its 'config' and provide plausible JSON examples for these fields. Explain that these are for testing only, and in live execution, the 'requestBody', 'requestHeaders', 'requestQuery' output handles are populated by the actual incoming HTTP request data from the API route. Also explain how they populate 'requestBody', 'requestHeaders', 'requestQuery' output handles respectively and include 'status' and 'error_message' output handles.], 'delay' [including 'delayMs' and that its 'output' handle passes input through], 'getEnvironmentVariable' [including 'variableName' and 'failIfNotSet' and its 'value', 'status', 'error_message' outputs]]) matches their requirements, including sub-flow or branch definitions, or field schemas and simulated responses for 'manualInput'. Include 'retry' or 'onErrorWebhook' configurations where robust error handling or notification is important. For 'retry', clearly define fields like 'attempts', 'delayMs', 'backoffFactor', 'retryOnStatusCodes' (for HTTP), and be very specific with examples for 'retryOnErrorKeywords' (case-insensitive, e.g., for an 'aiTask': ['timeout', 'quota exceeded', 'service unavailable', 'model unavailable', 'rate limit']; for 'databaseQuery': ['connection refused', 'deadlock', 'timeout']; for 'httpRequest': ['timeout', 'network error', 'connection reset', 'econnrefused']). For 'onErrorWebhook', explain potential uses like DLQ integration or triggering error-handling workflows, highlighting the utility of '{{workflow_data_snapshot_json}}'). Include simulation-specific config fields (e.g., 'simulatedResponse' (response body only) and 'simulatedStatusCode' (HTTP status) for 'httpRequest', 'simulatedOutput' for 'aiTask', 'simulated_config' for 'googleCalendarListEvents' (**which you MUST generate, e.g., an object with an 'events' key containing mock event objects**), 'simulatedResponse' for 'manualInput' (**which you must generate**), 'simulatedOutput' for 'callExternalWorkflow' (**which you must generate**), 'simulatedFileEvent' for 'fileSystemTrigger' (**which you must generate**), CRITICALLY 'simulatedRequestBody' (e.g., '{\"message\": \"Hello from webhook!\", \"userId\": \"user-sim-123\"}'), 'simulatedRequestHeaders' (e.g., '{\"X-Test\":\"true\"}'), 'simulatedRequestQuery' (e.g., '{\"sim\":\"on\"}') for 'webhookTrigger' (**which you MUST generate for each, noting they are for testing only; live runs use actual request data**), 'simulated_config' for new integration nodes (**which you MUST generate**)) for nodes that make external calls or represent external processes, to facilitate testing.
7.  Production Focus: Aim for robust, sensible workflows. Proactively include error handling for potentially fallible steps, even if not explicitly requested by the user, by using 'conditionalLogic' to check for 'status: "error"' and branching accordingly, or by configuring 'onErrorWebhook' for critical notifications. Include retry mechanisms where appropriate (leveraging 'retryOnErrorKeywords' for diverse errors and ensure they are case-insensitive). For 'whileLoop', consider if 'maxIterations' is suitable. For 'parallel', ensure branches are logically independent where possible and consider if a 'concurrencyLimit' is needed, and how branch data is scoped with 'inputMapping' and 'outputSource'. For 'forEach', decide if 'continueOnError' is appropriate for the use case (results will show individual statuses if true). For 'manualInput' steps, explain that its CRITICAL 'simulatedResponse' (**which you must generate**) (which becomes the node's 'output' data) allows the workflow to run end-to-end in the current system, but in a production scenario with pause/resume, this node would represent a point of actual human interaction (requiring external mechanisms for persistence and resumption not covered by the current simulation engine). For 'callExternalWorkflow', explain that its CRITICAL 'simulatedOutput' (**which you must generate**) is essential for its execution in the current simulated environment and how its 'output' handle (carrying mapped data derived from 'simulatedOutput' via 'outputMapping' using '{{calledWorkflow.property}}' placeholders), 'status', and 'error_message' handles should be used. For 'fileSystemTrigger', explain that its CRITICAL 'simulatedFileEvent' (**which you must generate**) (whose parsed content becomes data on the 'fileEvent' handle) is essential for its current simulated execution and how its 'fileEvent' handle should be used. For 'webhookTrigger', explain that its CRITICALLY required 'simulatedRequestBody', 'simulatedRequestHeaders', 'simulatedRequestQuery' (which **you must generate for each**) are essential for its current simulated execution and how its 'requestBody', 'requestHeaders', 'requestQuery', 'status', and 'error_message' handles should be used (noting they are populated by actual HTTP request data in live mode). The full webhook URL will be '/api/workflow-webhooks/[pathSuffix]', where pathSuffix is user-configured. For 'googleCalendarListEvents', explain that its CRITICAL 'simulated_config' (an object with an 'events' key containing mock events, **which you must generate in the config**) is essential for its current simulated execution, and its 'output.events' handle should be used. Also remind the user that actual execution would require Google OAuth credentials (configured via Credential Manager or GOOGLE_CLIENT_ID/GOOGLE_CLIENT_SECRET env vars as fallback). For new integration nodes like 'googleSheetsAppendRow', 'slackPostMessage', etc., you MUST include a 'simulated_config' object in their config with a plausible JSON object representing the successful output of the node in simulation mode.

Output Format:
The output MUST be a single, valid JSON object that strictly conforms to the Zod schema detailed below. Ensure all string values within the JSON are properly escaped to form a valid JSON document (e.g., double quotes inside strings should be like '\\"', backslashes as '\\\\', newlines as '\\n', etc.).

Schema Description:
${GenerateWorkflowFromPromptOutputSchema.description}
Nodes Schema: ${WorkflowNodeSchema.description}
Connections Schema: ${WorkflowConnectionSchema.description}

Analyze the user's request carefully and construct the complete workflow. Be meticulous about node types, configurations (with data flow placeholders, **active and detailed guidance in 'aiExplanation' for any user-required configurations following the "**Crucial for User Setup & AI Explanation**" guideline above, including actionable steps on how to obtain values like API keys or specific IDs, using '{{credential.USER_FRIENDLY_NAME}}' for managed credentials [explaining Credential Manager and env var fallback] or '{{env.A_DESCRIPTIVE_ENV_VAR}}' for direct env vars**, conditional execution fields [linking 'conditionalLogic' 'result' to '_flow_run_condition'], retry configurations including fields like 'attempts', 'delayMs', 'backoffFactor', 'retryOnStatusCodes', and diverse, case-insensitive 'retryOnErrorKeywords', 'onErrorWebhook' configurations including their potential use for DLQ/error workflow integration (especially using '{{workflow_data_snapshot_json}}'), CRITICAL simulation-related config fields like 'simulatedResponse' (response body only) and 'simulatedStatusCode' (HTTP status) for 'httpRequest', 'simulatedOutput' for 'aiTask', 'simulated_config' for 'googleCalendarListEvents' (**which you MUST generate, e.g., an object with an 'events' key containing mock event objects**), 'simulatedResponse' for 'manualInput' (**which you must generate**), 'simulatedOutput' for 'callExternalWorkflow' (**which you must generate**), 'simulatedFileEvent' for 'fileSystemTrigger' (**which you must generate**), CRITICALLY 'simulatedRequestBody' (e.g., '{\"message\": \"Hello from webhook!\", \"userId\": \"user-sim-123\"}'), 'simulatedRequestHeaders' (e.g., '{\"X-Test\":\"true\"}'), 'simulatedRequestQuery' (e.g., '{\"sim\":\"on\"}') for 'webhookTrigger' (**which you MUST generate for each, noting they are for testing only; live runs use actual request data**), 'simulated_config' for new integration nodes (**which you MUST generate**), and specific configs for 'executeFlowGroup' [including valid 'flowGroupNodes' with group-scoped unique IDs, 'flowGroupConnections', 'inputMapping' (how parent data is brought into group scope), and 'outputMapping' (how group data is exposed to parent)], 'forEach' [including 'inputArrayPath', 'iterationNodes' (with group-scoped unique IDs) with '{{item.property}}' usage for current item access, 'iterationConnections', optional 'iterationResultSource'. If 'continueOnError' is true, ensure the explanation mentions the structure of the 'results' array with individual statuses ('fulfilled' or 'rejected' with 'value' or 'reason') and the overall node status can be success/partial_success/error], 'whileLoop' [including 'condition' (evaluated against parent scope, modifiable by loop nodes), 'loopNodes' (with group-scoped unique IDs), 'loopConnections', 'maxIterations', and how loop state is managed], 'parallel' [including 'branch' structure with its individual branch 'id', 'nodes' (with group-scoped unique IDs), 'connections', branch-specific 'inputMapping' (how parent data is brought into branch scope), 'outputSource' (what part of branch execution data becomes its primary result), and optional 'concurrencyLimit' for the parent parallel node. The 'results' output is an object keyed by branch IDs, with values like '{status, value/reason}'], 'manualInput' [including 'instructions', 'inputFieldsSchema', and CRITICALLY 'simulatedResponse' (**ensure you generate a plausible JSON example for this in the config**) which determines its 'output' data, explaining its simulation nature in the current system vs. production pause/resume which is not handled by current engine], 'callExternalWorkflow' [including 'calledWorkflowId', 'inputMapping' (how data flows from current workflow to called one), 'outputMapping' (which uses '{{calledWorkflow.property}}' placeholders against the CRITICAL 'simulatedOutput' to determine the node's 'output' handle data), and CRITICALLY 'simulatedOutput' (**ensure you generate a plausible JSON example for this in the config**) (the JSON structure that 'outputMapping' references), and its 'output', 'status', 'error_message' handles], 'fileSystemTrigger' [including 'directoryPath', 'eventTypes', 'fileNamePattern', 'pollingIntervalSeconds', CRITICALLY 'simulatedFileEvent' (**ensure you generate a plausible JSON example for this in the config**) (whose parsed content becomes data on the 'fileEvent' handle), and its 'fileEvent', 'status', 'error_message' handles], 'webhookTrigger' [including 'pathSuffix' (and that full URL will be '/api/workflow-webhooks/[pathSuffix]' and is used for routing the incoming HTTP request to this specific workflow), optional 'securityToken' (e.g., '{{credential.MyWebhookToken}}' or '{{env.WEBHOOK_TOKEN}}'), CRITICALLY 'simulatedRequestBody' (e.g., '{\"message\": \"Hello from webhook!\", \"userId\": \"user-sim-123\"}'), 'simulatedRequestHeaders' (e.g., '{\"X-Test\":\"true\"}'), 'simulatedRequestQuery' (e.g., '{\"sim\":\"on\"}') in its 'config' and provide plausible JSON examples for these fields. Explain that these are for testing only, and in live execution, the 'requestBody', 'requestHeaders', 'requestQuery' output handles are populated by the actual incoming HTTP request data from the API route. Also explain how they populate 'requestBody', 'requestHeaders', 'requestQuery' output handles respectively and include 'status' and 'error_message' output handles.], 'delay' [including 'delayMs' and that its 'output' handle passes input through], 'getEnvironmentVariable' [including 'variableName' and 'failIfNotSet' and its 'value', 'status', 'error_message' outputs], **new integration nodes** [like 'googleSheetsAppendRow', 'slackPostMessage', etc., explaining their specific config fields like 'spreadsheetId', 'channel', etc., and the need for 'simulated_config' to be set with a plausible JSON object representing the successful output of the node in simulation mode, and the need for credentials like '{{credential.GoogleOAuth}}' or '{{credential.SlackBotToken}}' for live mode]])), and connections (with port/handle names where applicable). Explicitly design error paths using 'conditionalLogic' to check for 'status: "error"' from preceding nodes, and use its 'result' to drive '_flow_run_condition' in subsequent nodes, or use 'onErrorWebhook' for direct external error notification and DLQ integration. For 'manualInput' steps, explain that its CRITICAL 'simulatedResponse' (**which you must generate**) (which becomes the node's 'output' data) allows the workflow to run end-to-end in the current system, but in a production scenario with pause/resume, this node would represent a point of actual human interaction (requiring external mechanisms for persistence and resumption not covered by the current simulation engine). For 'callExternalWorkflow', explain that its CRITICAL 'simulatedOutput' (**which you must generate**) is essential for its execution in the current simulated environment and how its 'output' handle (carrying mapped data derived from 'simulatedOutput' via 'outputMapping' using '{{calledWorkflow.property}}' placeholders), 'status', and 'error_message' handles should be used. For 'fileSystemTrigger', explain that its CRITICAL 'simulatedFileEvent' (**which you must generate**) (whose parsed content becomes data on the 'fileEvent' handle) is essential for its current simulated execution and how its 'fileEvent' handle should be used. For 'webhookTrigger', explain that its CRITICALLY required 'simulatedRequestBody', 'simulatedRequestHeaders', 'simulatedRequestQuery' (which **you must generate for each**) are essential for its current simulated execution and how its 'requestBody', 'requestHeaders', 'requestQuery', 'status', and 'error_message' handles should be used (noting they are populated by actual HTTP request data in live mode). The full webhook URL will be '/api/workflow-webhooks/[pathSuffix]', where pathSuffix is user-configured. For 'googleCalendarListEvents', explain that its CRITICAL 'simulated_config' (an object with an 'events' key containing mock events, **which you must generate in the config**) is essential for its current simulated execution, and its 'output.events' handle should be used. Also remind the user that actual execution would require Google OAuth credentials (configured via Credential Manager or GOOGLE_CLIENT_ID/GOOGLE_CLIENT_SECRET env vars as fallback). For new integration nodes like 'googleSheetsAppendRow', 'slackPostMessage', etc., you MUST include a 'simulated_config' object in their config with a plausible JSON object representing the successful output of the node in simulation mode.
`
});

// Define the Genkit flow.
const generateWorkflowFromPromptFlow = ai.defineFlow(
  {
    name: 'generateWorkflowFromPromptFlow',
    inputSchema: GenerateWorkflowFromPromptInputSchema,
    outputSchema: GenerateWorkflowFromPromptOutputSchema,
  },
  async input => {
    const {output} = await generateWorkflowPrompt(input);
    if (!output) {
        throw new Error("AI failed to generate a workflow. Output was null.");
    }
    if (output.nodes.length === 0 && input.prompt.trim() !== "") {
        console.warn("AI generated an empty workflow for a non-empty prompt.");
    }

    // Filter out any nodes that are empty objects or missing essential fields
    const validNodes = output.nodes.filter(node => {
        // Check if node is not an empty object and has at least id, type, and position
        if (typeof node === 'object' && node !== null && node.id && node.type && node.position) {
            return true;
        }
        console.warn("AI generated an invalid/empty node object, filtering out:", node);
        return false;
    });
    
    // Ensure connections have IDs if AI doesn't provide them
    const connectionsWithIds = output.connections.map(conn => ({
        ...conn,
        id: conn.id || crypto.randomUUID()
    }));

    return { ...output, nodes: validNodes, connections: connectionsWithIds };
  }
);
